{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"private_outputs":true,"provenance":[],"toc_visible":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Ungraded Lab: Generating Sequences and Padding\n\nIn this lab, you will look at converting input sentences into numeric sequences. Similar to images in the previous course, you need to prepare text data with uniform size before feeding it to your model. You will see how to do these in the next sections.","metadata":{"id":"1SmE2CODfmmL"}},{"cell_type":"markdown","source":"## Text to Sequences\n\nIn the previous lab, you saw how to use the `TextVectorization` layer to build a vocabulary from your corpus. It generates a list where more frequent words have lower indices.","metadata":{"id":"JiFUJg-lmTm6"}},{"cell_type":"code","source":"import tensorflow as tf\n\n# Sample inputs\nsentences = [\n    'I love my dog',\n    'I love my cat',\n    'You love my dog!',\n    'Do you think my dog is amazing?'\n    ]\n\n# Initialize the layer\nvectorize_layer = tf.keras.layers.TextVectorization()\n\n# Compute the vocabulary\nvectorize_layer.adapt(sentences)\n\n# Get the vocabulary\nvocabulary = vectorize_layer.get_vocabulary()\n\n# Print the token index\nfor index, word in enumerate(vocabulary):\n  print(index, word)","metadata":{"id":"LXzsIYWMvFM-","trusted":true,"execution":{"iopub.status.busy":"2025-02-04T17:02:54.403080Z","iopub.execute_input":"2025-02-04T17:02:54.403477Z","iopub.status.idle":"2025-02-04T17:03:10.119581Z","shell.execute_reply.started":"2025-02-04T17:02:54.403422Z","shell.execute_reply":"2025-02-04T17:03:10.118503Z"}},"outputs":[{"name":"stdout","text":"0 \n1 [UNK]\n2 my\n3 love\n4 dog\n5 you\n6 i\n7 think\n8 is\n9 do\n10 cat\n11 amazing\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"You can then use the result to convert each of the input sentences into integer sequences. See how that's done below given a single input string.","metadata":{"id":"0VNFxYidr9qg"}},{"cell_type":"code","source":"# String input\nsample_input = 'I love my dog'\n\n# Convert the string input to an integer sequence\nsequence = vectorize_layer(sample_input)\n\n# Print the result\nprint(sequence)","metadata":{"id":"lQWcXlE1saUS","trusted":true,"execution":{"iopub.status.busy":"2025-02-04T17:03:45.089550Z","iopub.execute_input":"2025-02-04T17:03:45.089953Z","iopub.status.idle":"2025-02-04T17:03:45.122544Z","shell.execute_reply.started":"2025-02-04T17:03:45.089923Z","shell.execute_reply":"2025-02-04T17:03:45.121527Z"}},"outputs":[{"name":"stdout","text":"tf.Tensor([6 3 2 4], shape=(4,), dtype=int64)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"As shown, you simply pass in the string to the layer which already learned the vocabulary, and it will output the integer sequence as a `tf.Tensor`. In this case, the result is `[6 3 2 4]`. You can look at the token index printed above to verify that it matches the indices for each word in the input string.\n\nFor a given list of string inputs (such as the 4-item `sentences` list above), you will need to apply the layer to each input. There's more than one way to do this. Let's first use the `map()` method and see the results.","metadata":{"id":"6ZZnENfZtoiA"}},{"cell_type":"code","source":"# Convert the list to tf.data.Dataset\nsentences_dataset = tf.data.Dataset.from_tensor_slices(sentences)\n\n# Define a mapping function to convert each sample input\nsequences = sentences_dataset.map(vectorize_layer)\n\n# Print the integer sequences\nfor sentence, sequence in zip(sentences, sequences):\n  print(f'{sentence} ---> {sequence}')","metadata":{"id":"41foIDBQw3FA","trusted":true,"execution":{"iopub.status.busy":"2025-02-04T17:04:22.221952Z","iopub.execute_input":"2025-02-04T17:04:22.222462Z","iopub.status.idle":"2025-02-04T17:04:22.393054Z","shell.execute_reply.started":"2025-02-04T17:04:22.222423Z","shell.execute_reply":"2025-02-04T17:04:22.391991Z"}},"outputs":[{"name":"stdout","text":"I love my dog ---> [6 3 2 4]\nI love my cat ---> [ 6  3  2 10]\nYou love my dog! ---> [5 3 2 4]\nDo you think my dog is amazing? ---> [ 9  5  7  2  4  8 11]\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"As you can see, each sentence is successfully transformed into an integer sequence. The problem with this is they have varying lengths so it cannot be consumed by the model right away.","metadata":{"id":"yV_91IQB62R0"}},{"cell_type":"markdown","source":"## Padding\n\nYou can get a list of varying lengths to have a uniform size by padding or truncating tokens from the sequences. Padding is more common to preserve information.\n\nRecall that your vocabulary reserves a special token index `0` for padding. It will add that token (called post padding) if you pass in a list of string inputs to the layer. See an example below. Notice that you have the same output as above but the integer sequences are already post-padded with `0` up to the length of the longest sequence.","metadata":{"id":"z56pEkF2p8c-"}},{"cell_type":"code","source":"# Apply the layer to the string input list\nsequences_post = vectorize_layer(sentences)\n\n# Print the results\nprint('INPUT:')\nprint(sentences)\nprint()\n\nprint('OUTPUT:')\nprint(sequences_post)","metadata":{"id":"DJpjZvG9wtLP","trusted":true,"execution":{"iopub.status.busy":"2025-02-04T17:04:32.615099Z","iopub.execute_input":"2025-02-04T17:04:32.615419Z","iopub.status.idle":"2025-02-04T17:04:32.629567Z","shell.execute_reply.started":"2025-02-04T17:04:32.615395Z","shell.execute_reply":"2025-02-04T17:04:32.628498Z"}},"outputs":[{"name":"stdout","text":"INPUT:\n['I love my dog', 'I love my cat', 'You love my dog!', 'Do you think my dog is amazing?']\n\nOUTPUT:\ntf.Tensor(\n[[ 6  3  2  4  0  0  0]\n [ 6  3  2 10  0  0  0]\n [ 5  3  2  4  0  0  0]\n [ 9  5  7  2  4  8 11]], shape=(4, 7), dtype=int64)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"If you want pre-padding, you can use the [pad_sequences()](https://www.tensorflow.org/api_docs/python/tf/keras/utils/pad_sequences) utility to prepend a padding token to the sequences. Notice that the `padding` argument is set to `pre`. This is just for clarity. The function already has this set as the default so you can opt to drop it.","metadata":{"id":"EHqYAVmNAi5D"}},{"cell_type":"code","source":"# Pre-pad the sequences to a uniform length.\n# You can remove the `padding` argument and get the same result.\nsequences_pre = tf.keras.utils.pad_sequences(sequences, padding='pre')\n\n# Print the results\nprint('INPUT:')\n[print(sequence.numpy()) for sequence in sequences]\nprint()\n\nprint('OUTPUT:')\nprint(sequences_pre)","metadata":{"id":"qljgx1eSlEse","trusted":true,"execution":{"iopub.status.busy":"2025-02-04T17:07:58.296365Z","iopub.execute_input":"2025-02-04T17:07:58.296708Z","iopub.status.idle":"2025-02-04T17:07:58.386559Z","shell.execute_reply.started":"2025-02-04T17:07:58.296683Z","shell.execute_reply":"2025-02-04T17:07:58.385728Z"}},"outputs":[{"name":"stdout","text":"INPUT:\n[6 3 2 4]\n[ 6  3  2 10]\n[5 3 2 4]\n[ 9  5  7  2  4  8 11]\n\nOUTPUT:\n[[ 0  0  0  6  3  2  4]\n [ 0  0  0  6  3  2 10]\n [ 0  0  0  5  3  2  4]\n [ 9  5  7  2  4  8 11]]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"If you switch the `padding` argument to `post`, you will arrive at the same result as applying the layer directly.\n\nThe function also has a `maxlen` argument that you can use to truncate tokens from the sequences. By default, it will drop tokens in front. If you want to drop tokens at the other end, you will have to set the `truncating` argument to `post`.","metadata":{"id":"GsgHsYP2DDrQ"}},{"cell_type":"code","source":"# Post-pad the sequences and limit the size to 5.\nsequences_post_trunc = tf.keras.utils.pad_sequences(sequences, maxlen=5, padding='pre')\n\n# Print the results\nprint('INPUT:')\n[print(sequence.numpy()) for sequence in sequences]\nprint()\n\nprint('OUTPUT:')\nprint(sequences_post_trunc)","metadata":{"id":"70dmSqBDAbYH","trusted":true,"execution":{"iopub.status.busy":"2025-02-04T17:09:57.402264Z","iopub.execute_input":"2025-02-04T17:09:57.402597Z","iopub.status.idle":"2025-02-04T17:09:57.489303Z","shell.execute_reply.started":"2025-02-04T17:09:57.402571Z","shell.execute_reply":"2025-02-04T17:09:57.488407Z"}},"outputs":[{"name":"stdout","text":"INPUT:\n[6 3 2 4]\n[ 6  3  2 10]\n[5 3 2 4]\n[ 9  5  7  2  4  8 11]\n\nOUTPUT:\n[[ 0  6  3  2  4]\n [ 0  6  3  2 10]\n [ 0  5  3  2  4]\n [ 7  2  4  8 11]]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"Another way to prepare your sequences for prepadding is to set the TextVectorization to output a ragged tensor. This means the output will not be automatically post-padded. See the output sequences here.","metadata":{}},{"cell_type":"code","source":"# Set the layer to output a ragged tensor\nvectorize_layer = tf.keras.layers.TextVectorization(ragged=True)\n\n# Compute the vocabulary\nvectorize_layer.adapt(sentences)\n\n# Apply the layer to the sentences\nragged_sequences = vectorize_layer(sentences)\n\n# Print the results\nprint(ragged_sequences)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T17:10:23.145545Z","iopub.execute_input":"2025-02-04T17:10:23.145926Z","iopub.status.idle":"2025-02-04T17:10:23.172170Z","shell.execute_reply.started":"2025-02-04T17:10:23.145893Z","shell.execute_reply":"2025-02-04T17:10:23.171102Z"}},"outputs":[{"name":"stdout","text":"<tf.RaggedTensor [[6, 3, 2, 4], [6, 3, 2, 10], [5, 3, 2, 4], [9, 5, 7, 2, 4, 8, 11]]>\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"With that, you can now pass it directly to the `pad_sequences()` utility.","metadata":{}},{"cell_type":"code","source":"# Pre-pad the sequences in the ragged tensor\nsequences_pre = tf.keras.utils.pad_sequences(ragged_sequences.numpy())\n\n# Print the results\nprint(sequences_pre)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T17:11:44.211076Z","iopub.execute_input":"2025-02-04T17:11:44.211420Z","iopub.status.idle":"2025-02-04T17:11:44.218102Z","shell.execute_reply.started":"2025-02-04T17:11:44.211395Z","shell.execute_reply":"2025-02-04T17:11:44.216904Z"}},"outputs":[{"name":"stdout","text":"[[ 0  0  0  6  3  2  4]\n [ 0  0  0  6  3  2 10]\n [ 0  0  0  5  3  2  4]\n [ 9  5  7  2  4  8 11]]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## Out-of-vocabulary tokens\n\nLastly, you'll see what the other special token is for. The layer will use the token index `1` when you have input words that are not found in the vocabulary list. For example, you may decide to collect more text after your initial training and decide to not recompute the vocabulary. You will see this in action in the cell below. Notice that the token `1` is inserted for words that are not found in the list.","metadata":{"id":"btEb9jI0k7Ip"}},{"cell_type":"code","source":"# Try with words that are not in the vocabulary\nsentences_with_oov = [\n    'i really love my dog',\n    'my dog loves my manatee'\n]\n\n# Generate the sequences\nsequences_with_oov = vectorize_layer(sentences_with_oov)\n\n# Print the integer sequences\nfor sentence, sequence in zip(sentences_with_oov, sequences_with_oov):\n  print(f'{sentence} ---> {sequence}')","metadata":{"id":"4fW1NWTok72V","trusted":true,"execution":{"iopub.status.busy":"2025-02-04T17:12:59.227608Z","iopub.execute_input":"2025-02-04T17:12:59.227981Z","iopub.status.idle":"2025-02-04T17:12:59.247117Z","shell.execute_reply.started":"2025-02-04T17:12:59.227955Z","shell.execute_reply":"2025-02-04T17:12:59.245817Z"}},"outputs":[{"name":"stdout","text":"i really love my dog ---> [6 1 3 2 4]\nmy dog loves my manatee ---> [2 4 1 2 1]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"This concludes another introduction to text data preprocessing. So far, you've just been using dummy data. In the next exercise, you will be applying the same concepts to a real-world and much larger dataset.","metadata":{"id":"UBlQIPBqskAJ"}}]}