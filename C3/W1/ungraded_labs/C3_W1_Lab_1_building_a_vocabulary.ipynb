{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6ef3f15",
   "metadata": {
    "id": "rL-LzAqpoGLC",
    "papermill": {
     "duration": 0.003431,
     "end_time": "2025-02-04T16:58:45.677680",
     "exception": false,
     "start_time": "2025-02-04T16:58:45.674249",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Ungraded Lab: Building a Vocabulary 构建词汇表\n",
    "\n",
    "In most natural language processing (NLP) tasks, the initial step in preparing your data is to extract a vocabulary of words from your corpus (i.e. input texts). You will need to define how to represent the texts into numeric features which can be used to train a neural network. Tensorflow and Keras makes it easy to generate these using its APIs. You will see how to do that in the next cells.\n",
    "\n",
    "在大多数自然语言处理（NLP）任务中，准备数据的第一步是从语料库（即输入文本）中提取词汇表。你需要定义如何将文本表示为数值特征，以便用于训练神经网络。TensorFlow 和 Keras 提供了便捷的 API 来实现这一过程。在接下来的单元格中，你将看到如何操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f161c7",
   "metadata": {
    "id": "-nt3uR9TPrUt",
    "papermill": {
     "duration": 0.002463,
     "end_time": "2025-02-04T16:58:45.683252",
     "exception": false,
     "start_time": "2025-02-04T16:58:45.680789",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The code below takes a list of sentences, then takes each word in those sentences and assigns it to an integer. This is done using the [TextVectorization()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) preprocessing layer and its [adapt()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization#adapt) method.\n",
    "\n",
    "As mentioned in the docs above, this layer does several things including:\n",
    "\n",
    "1. Standardizing each example. The default behavior is to lowercase and strip punctuation. See its `standardize` argument for other options.\n",
    "2. Splitting each example into substrings. By default, it will split into words. See its `split` argument for other options.\n",
    "3. Recombining substrings into tokens. See its `ngrams` argument for reference.\n",
    "4. Indexing tokens.\n",
    "5. Transforming each example using this index, either into a vector of ints or a dense float vector.\n",
    "\n",
    "Run the cells below to see this in action.\n",
    "\n",
    "下面的代码会处理一个句子列表，将其中的每个单词转换为对应的整数索引。这是通过 TextVectorization() 预处理层及其 adapt() 方法来实现的。\n",
    "\n",
    "根据上述文档，这个层执行多个操作，包括：\n",
    "\n",
    "标准化 每个样本。默认行为是转换为小写并去除标点符号。可以使用 standardize 参数调整此行为。\n",
    "拆分 每个样本为子字符串。默认情况下按单词拆分，可使用 split 参数自定义拆分方式。\n",
    "重新组合 子字符串为标记（token），可以参考 ngrams 参数来调整。\n",
    "索引化 标记（token）。\n",
    "转换 每个样本，使用索引将文本表示为整数向量或密集浮点向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "583d3903",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T16:58:45.690068Z",
     "iopub.status.busy": "2025-02-04T16:58:45.689707Z",
     "iopub.status.idle": "2025-02-04T16:59:02.833955Z",
     "shell.execute_reply": "2025-02-04T16:59:02.832841Z"
    },
    "id": "CLFb7wXVTQXc",
    "papermill": {
     "duration": 17.149981,
     "end_time": "2025-02-04T16:59:02.835969",
     "exception": false,
     "start_time": "2025-02-04T16:58:45.685988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Sample inputs\n",
    "sentences = [\n",
    "    'i love my dog',\n",
    "    'I, love my cat'\n",
    "    ]\n",
    "\n",
    "# Initialize the layer\n",
    "vectorize_layer = tf.keras.layers.TextVectorization()\n",
    "\n",
    "# Build the vocabulary\n",
    "vectorize_layer.adapt(sentences)\n",
    "\n",
    "# Get the vocabulary list. Ignore special tokens for now.\n",
    "vocabulary = vectorize_layer.get_vocabulary(include_special_tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04b6464",
   "metadata": {
    "id": "FNmxq2MMYbDc",
    "papermill": {
     "duration": 0.002698,
     "end_time": "2025-02-04T16:59:02.841840",
     "exception": false,
     "start_time": "2025-02-04T16:59:02.839142",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The resulting `vocabulary` will be a list where more frequently used words will have a lower index. By default, it will also reserve indices for special tokens but , for clarity, let's reserve that for later.\n",
    "\n",
    "生成的 vocabulary（词汇表）将是一个列表，其中更常用的单词会被分配较低的索引。默认情况下，该层还会为特殊标记保留索引，但为了清晰起见，我们暂时不考虑这一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bb25550",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T16:59:02.848838Z",
     "iopub.status.busy": "2025-02-04T16:59:02.848473Z",
     "iopub.status.idle": "2025-02-04T16:59:02.855874Z",
     "shell.execute_reply": "2025-02-04T16:59:02.854871Z"
    },
    "id": "W-uJ4K_ts956",
    "papermill": {
     "duration": 0.013042,
     "end_time": "2025-02-04T16:59:02.857700",
     "exception": false,
     "start_time": "2025-02-04T16:59:02.844658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 my\n",
      "1 love\n",
      "2 i\n",
      "3 dog\n",
      "4 cat\n"
     ]
    }
   ],
   "source": [
    "# Print the token index\n",
    "for index, word in enumerate(vocabulary):\n",
    "  print(index, word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d84d808",
   "metadata": {
    "id": "9vFOmuRbZUes",
    "papermill": {
     "duration": 0.002656,
     "end_time": "2025-02-04T16:59:02.863469",
     "exception": false,
     "start_time": "2025-02-04T16:59:02.860813",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "If you add another sentence, you'll notice new words in the vocabulary and new punctuation is still ignored as expected.\n",
    "\n",
    "\n",
    "如果你添加另一句话，你会发现词汇表中包含了新的单词，同时新的标点符号仍然会被忽略，这是预期的行为。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e5021c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T16:59:02.870649Z",
     "iopub.status.busy": "2025-02-04T16:59:02.870167Z",
     "iopub.status.idle": "2025-02-04T16:59:02.888351Z",
     "shell.execute_reply": "2025-02-04T16:59:02.887108Z"
    },
    "id": "VX1A1pDNoVKm",
    "papermill": {
     "duration": 0.023927,
     "end_time": "2025-02-04T16:59:02.890377",
     "exception": false,
     "start_time": "2025-02-04T16:59:02.866450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add another input\n",
    "sentences = [\n",
    "    'i love my dog',\n",
    "    'I, love my cat',\n",
    "    'You love my dog!'\n",
    "]\n",
    "\n",
    "# Initialize the layer\n",
    "vectorize_layer = tf.keras.layers.TextVectorization()\n",
    "\n",
    "# Build the vocabulary\n",
    "vectorize_layer.adapt(sentences)\n",
    "\n",
    "# Get the vocabulary list. Ignore special tokens for now.\n",
    "vocabulary = vectorize_layer.get_vocabulary(include_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49668e6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T16:59:02.898016Z",
     "iopub.status.busy": "2025-02-04T16:59:02.897568Z",
     "iopub.status.idle": "2025-02-04T16:59:02.904445Z",
     "shell.execute_reply": "2025-02-04T16:59:02.903521Z"
    },
    "id": "NvM_N6J0tGAM",
    "papermill": {
     "duration": 0.012754,
     "end_time": "2025-02-04T16:59:02.906384",
     "exception": false,
     "start_time": "2025-02-04T16:59:02.893630",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 my\n",
      "1 love\n",
      "2 i\n",
      "3 dog\n",
      "4 you\n",
      "5 cat\n"
     ]
    }
   ],
   "source": [
    "# Print the token index\n",
    "for index, word in enumerate(vocabulary):\n",
    "  print(index, word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebab6dab",
   "metadata": {
    "id": "VoUPdaR6bIO-",
    "papermill": {
     "duration": 0.002718,
     "end_time": "2025-02-04T16:59:02.912260",
     "exception": false,
     "start_time": "2025-02-04T16:59:02.909542",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now that you see how it behaves, let's include the two special tokens. The first one at `0` is used for padding and `1` is used for out-of-vocabulary words. These are important when you use the layer to convert input texts to integer sequences. You'll see that in the next lab.\n",
    "\n",
    "现在你已经了解了它的行为，让我们加入两个特殊标记。索引 0 用于填充（padding），索引 1 用于未登录词（out-of-vocabulary, OOV）。当你使用该层将输入文本转换为整数序列时，这些标记会很重要。你将在下一个实验中看到它们的作用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3329686a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T16:59:02.919641Z",
     "iopub.status.busy": "2025-02-04T16:59:02.919206Z",
     "iopub.status.idle": "2025-02-04T16:59:02.929598Z",
     "shell.execute_reply": "2025-02-04T16:59:02.928428Z"
    },
    "id": "1NJNJZ8SQ3pM",
    "papermill": {
     "duration": 0.016316,
     "end_time": "2025-02-04T16:59:02.931633",
     "exception": false,
     "start_time": "2025-02-04T16:59:02.915317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      "1 [UNK]\n",
      "2 my\n",
      "3 love\n",
      "4 i\n",
      "5 dog\n",
      "6 you\n",
      "7 cat\n"
     ]
    }
   ],
   "source": [
    "# Get the vocabulary list.\n",
    "vocabulary = vectorize_layer.get_vocabulary()\n",
    "\n",
    "# Print the token index\n",
    "for index, word in enumerate(vocabulary):\n",
    "  print(index, word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb099dd6",
   "metadata": {
    "id": "GtY2P9HmkqWl",
    "papermill": {
     "duration": 0.003177,
     "end_time": "2025-02-04T16:59:02.938162",
     "exception": false,
     "start_time": "2025-02-04T16:59:02.934985",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "That concludes this short exercise on building a vocabulary!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 22.627108,
   "end_time": "2025-02-04T16:59:05.341067",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-04T16:58:42.713959",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
