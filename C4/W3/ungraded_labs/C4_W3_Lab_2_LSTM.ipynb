{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"C4_W3_Lab_2_LSTM.ipynb","private_outputs":true,"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Ungraded Lab: Using a multi-layer LSTM for forecasting\n\nIn this lab, you will use the same RNN architecure in the first lab but will instead stack [LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM) layers instead of `SimpleRNN`.","metadata":{"id":"sV_nvXSQGJPK"}},{"cell_type":"markdown","source":"## Imports","metadata":{"id":"3IMbAWETGOWD"}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"id":"BOjujz601HcS","trusted":true,"execution":{"iopub.status.busy":"2025-03-07T23:29:39.687210Z","iopub.execute_input":"2025-03-07T23:29:39.687479Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Utilities","metadata":{"id":"kD7RQQ0xGQVH"}},{"cell_type":"code","source":"def plot_series(time, series, format=\"-\", start=0, end=None):\n    \"\"\"\n    Visualizes time series data\n\n    Args:\n      time (array of int) - contains the time steps\n      series (array of int) - contains the measurements for each time step\n      format - line style when plotting the graph\n      start - first time step to plot\n      end - last time step to plot\n    \"\"\"\n\n    # Setup dimensions of the graph figure\n    plt.figure(figsize=(10, 6))\n    \n    if type(series) is tuple:\n\n      for series_num in series:\n        # Plot the time series data\n        plt.plot(time[start:end], series_num[start:end], format)\n\n    else:\n      # Plot the time series data\n      plt.plot(time[start:end], series[start:end], format)\n\n    # Label the x-axis\n    plt.xlabel(\"Time\")\n\n    # Label the y-axis\n    plt.ylabel(\"Value\")\n\n    # Overlay a grid on the graph\n    plt.grid(True)\n\n    # Draw the graph on screen\n    plt.show()\n\ndef trend(time, slope=0):\n    \"\"\"\n    Generates synthetic data that follows a straight line given a slope value.\n\n    Args:\n      time (array of int) - contains the time steps\n      slope (float) - determines the direction and steepness of the line\n\n    Returns:\n      series (array of float) - measurements that follow a straight line\n    \"\"\"\n\n    # Compute the linear series given the slope\n    series = slope * time\n\n    return series\n\ndef seasonal_pattern(season_time):\n    \"\"\"\n    Just an arbitrary pattern, you can change it if you wish\n    \n    Args:\n      season_time (array of float) - contains the measurements per time step\n\n    Returns:\n      data_pattern (array of float) -  contains revised measurement values according \n                                  to the defined pattern\n    \"\"\"\n\n    # Generate the values using an arbitrary pattern\n    data_pattern = np.where(season_time < 0.4,\n                    np.cos(season_time * 2 * np.pi),\n                    1 / np.exp(3 * season_time))\n    \n    return data_pattern\n\ndef seasonality(time, period, amplitude=1, phase=0):\n    \"\"\"\n    Repeats the same pattern at each period\n\n    Args:\n      time (array of int) - contains the time steps\n      period (int) - number of time steps before the pattern repeats\n      amplitude (int) - peak measured value in a period\n      phase (int) - number of time steps to shift the measured values\n\n    Returns:\n      data_pattern (array of float) - seasonal data scaled by the defined amplitude\n    \"\"\"\n    \n    # Define the measured values per period\n    season_time = ((time + phase) % period) / period\n\n    # Generates the seasonal data scaled by the defined amplitude\n    data_pattern = amplitude * seasonal_pattern(season_time)\n\n    return data_pattern\n\ndef noise(time, noise_level=1, seed=None):\n    \"\"\"Generates a normally distributed noisy signal\n\n    Args:\n      time (array of int) - contains the time steps\n      noise_level (float) - scaling factor for the generated signal\n      seed (int) - number generator seed for repeatability\n\n    Returns:\n      noise (array of float) - the noisy signal\n    \"\"\"\n\n    # Initialize the random number generator\n    rnd = np.random.RandomState(seed)\n\n    # Generate a random number for each time step and scale by the noise level\n    noise = rnd.randn(len(time)) * noise_level\n    \n    return noise","metadata":{"id":"Zswl7jRtGzkk","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Generate the Synthetic Data","metadata":{"id":"yxNyeFWjGSdj"}},{"cell_type":"code","source":"# Parameters\ntime = np.arange(4 * 365 + 1, dtype=\"float32\")\nbaseline = 10\namplitude = 40\nslope = 0.05\nnoise_level = 5\n\n# Create the series\nseries = baseline + trend(time, slope) + seasonality(time, period=365, amplitude=amplitude)\n\n# Update with noise\nseries += noise(time, noise_level, seed=42)\n\n# Plot the results\nplot_series(time, series)","metadata":{"id":"KYEUfDbdpHPm","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Split the Dataset","metadata":{"id":"MYKCvv-eGWRc"}},{"cell_type":"code","source":"# Define the split time\nsplit_time = 1000\n\n# Get the train set \ntime_train = time[:split_time]\nx_train = series[:split_time]\n\n# Get the validation set\ntime_valid = time[split_time:]\nx_valid = series[split_time:]","metadata":{"id":"hpp0slenpKVD","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Prepare Features and Labels","metadata":{"id":"cktHz9aOGYtV"}},{"cell_type":"code","source":"# Parameters\nwindow_size = 20\nbatch_size = 32\nshuffle_buffer_size = 1000","metadata":{"id":"G_j_2_Mqwn7-","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n    \"\"\"Generates dataset windows\n\n    Args:\n      series (array of float) - contains the values of the time series\n      window_size (int) - the number of time steps to include in the feature\n      batch_size (int) - the batch size\n      shuffle_buffer(int) - buffer size to use for the shuffle method\n\n    Returns:\n      dataset (TF Dataset) - TF Dataset containing time windows\n    \"\"\"\n\n    # Add an axis for the feature dimension of RNN layers\n    series = tf.expand_dims(series, axis=-1)\n\n    # Generate a TF Dataset from the series values\n    dataset = tf.data.Dataset.from_tensor_slices(series)\n    \n    # Window the data but only take those with the specified size\n    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n    \n    # Flatten the windows by putting its elements in a single batch\n    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n\n    # Create tuples with features and labels \n    dataset = dataset.map(lambda window: (window[:-1], window[-1]))\n\n    # Shuffle the windows\n    dataset = dataset.shuffle(shuffle_buffer)\n    \n    # Create batches of windows\n    dataset = dataset.batch(batch_size)\n    \n    # Optimize the dataset for training\n    dataset = dataset.cache().prefetch(1)\n    \n    return dataset","metadata":{"id":"4sTTIOCbyShY","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate the dataset windows\ndataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)","metadata":{"id":"vGj5-InSwtQQ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Build the Model\n\nAs mentioned, you will swap `SimpleRNN` for `LSTM` in this lab. It is also set as bidirectional below but feel free to revise later and see what results you get. LSTMs are much more complex in their internal architecture than simpleRNNs. It implements a cell state that allows it to remember sequences better than simple implementations. This added complexity results in a bigger set of parameters to train and you'll see that when you print the model summary below.","metadata":{"id":"p9-Ke8ZaGcjd"}},{"cell_type":"code","source":"# Build the Model\nmodel_tune = tf.keras.models.Sequential([\n    tf.keras.Input(shape=(window_size, 1)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    tf.keras.layers.Dense(1),\n    tf.keras.layers.Lambda(lambda x: x * 100.0)\n])\n\n# Print the model summary\nmodel_tune.summary()","metadata":{"id":"A1Hl39rklkLm","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Tune the Learning Rate\n\nAs usual, you will pick a learning rate by running the tuning code below.","metadata":{"id":"FHaFblbwq0GV"}},{"cell_type":"code","source":"# Set the learning rate scheduler\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(\n    lambda epoch: 1e-8 * 10**(epoch / 20))\n\n# Initialize the optimizer\noptimizer = tf.keras.optimizers.SGD(momentum=0.9)\n\n# Set the training parameters\nmodel_tune.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer)\n\n# Train the model\nhistory = model_tune.fit(dataset, epochs=100, callbacks=[lr_schedule])","metadata":{"id":"qE7al18qw48G","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the learning rate array\nlrs = 1e-8 * (10 ** (np.arange(100) / 20))\n\n# Set the figure size\nplt.figure(figsize=(10, 6))\n\n# Set the grid\nplt.grid(True)\n\n# Plot the loss in log scale\nplt.semilogx(lrs, history.history[\"loss\"])\n\n# Increase the tickmarks size\nplt.tick_params('both', length=10, width=1, which='both')\n\n# Set the plot boundaries\nplt.axis([1e-8, 1e-3, 0, 30])","metadata":{"id":"AkBsrsXMzoWR","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train the Model\n\nYou can then proceed to train the model with your chosen learning rate. \n\n*Tip: When experimenting and you find yourself running different iterations of a model, you may want to use the [`clear_session()`](https://www.tensorflow.org/api_docs/python/tf/keras/backend/clear_session) method to declutter memory used by Keras. This is added in the first line below.*\n","metadata":{"id":"i-lKNp4pq_w8"}},{"cell_type":"code","source":"# Reset states generated by Keras\ntf.keras.backend.clear_session()\n\n# Build the model\nmodel = tf.keras.models.Sequential([\n    tf.keras.Input(shape=(window_size, 1)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    tf.keras.layers.Dense(1),\n    tf.keras.layers.Lambda(lambda x: x * 100.0)\n])\n\n# Set the learning rate\nlearning_rate = 2e-6\n\n# Set the optimizer \noptimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n\n# Set the training parameters\nmodel.compile(loss=tf.keras.losses.Huber(),\n              optimizer=optimizer,\n              metrics=[\"mae\"])\n\n# Train the model\nhistory = model.fit(dataset,epochs=100)","metadata":{"id":"4uh-97bpLZCA","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Prediction\n\nYou will then generate batches of windows to generate predictions that align with the validation set.","metadata":{"id":"RjP4kbGiI7cw"}},{"cell_type":"code","source":"def model_forecast(model, series, window_size, batch_size):\n    \"\"\"Uses an input model to generate predictions on data windows\n\n    Args:\n      model (TF Keras Model) - model that accepts data windows\n      series (array of float) - contains the values of the time series\n      window_size (int) - the number of time steps to include in the window\n      batch_size (int) - the batch size\n\n    Returns:\n      forecast (numpy array) - array containing predictions\n    \"\"\"\n\n    # Add an axis for the feature dimension of RNN layers\n    series = tf.expand_dims(series, axis=-1)\n    \n    # Generate a TF Dataset from the series values\n    dataset = tf.data.Dataset.from_tensor_slices(series)\n\n    # Window the data but only take those with the specified size\n    dataset = dataset.window(window_size, shift=1, drop_remainder=True)\n\n    # Flatten the windows by putting its elements in a single batch\n    dataset = dataset.flat_map(lambda w: w.batch(window_size))\n    \n    # Create batches of windows\n    dataset = dataset.batch(batch_size).prefetch(1)\n    \n    # Get predictions on the entire dataset\n    forecast = model.predict(dataset, verbose=0)\n    \n    return forecast","metadata":{"id":"QO8cfKwKBBG6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Reduce the original series\nforecast_series = series[split_time-window_size:-1]\n\n# Use helper function to generate predictions\nforecast = model_forecast(model, forecast_series, window_size, batch_size)\n\n# Drop single dimensional axis\nresults = forecast.squeeze()\n\n# Plot the results\nplot_series(time_valid, (x_valid, results))","metadata":{"id":"_plB3UseBD8o","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"You can then generate the metrics to evaluate the model's performance.","metadata":{"id":"Nn8iSQkZtaRC"}},{"cell_type":"code","source":"# Compute the MSE and MAE\nprint(tf.keras.metrics.mse(x_valid, results).numpy())\nprint(tf.keras.metrics.mae(x_valid, results).numpy())","metadata":{"id":"-IKhueZaBGID","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Wrap Up\n\nThis concludes this short exercise on using LSTMs for time series forecasting. Next week, you will build upon this and add convolutions. Then, you will start to move away from synthetic data and use real-world datasets. See you there!","metadata":{"id":"bzPIqeMWvbPy"}},{"cell_type":"markdown","source":"## Optional: Including a Validation Set while Training\n\nBack in the first course of this specialization, you saw how you can also monitor the performance of your model against a validation set while training. You can also do that for this lab. \n\nFirst, you need to generate a `val_set` which are data windows and labels that your model can accept. You can simply reuse the `windowed_dataset` function for that and you can pass in the `x_valid` points to generate the windows.","metadata":{"id":"U5PqTePGHeMH"}},{"cell_type":"code","source":"# Generate data windows of the validation set\nval_set = windowed_dataset(x_valid, window_size, batch_size, shuffle_buffer_size)","metadata":{"id":"kvFVwL3PC4iX","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"You can then do the same training as before but pass in the `val_set` to the `validation_data` parameter of the `fit()` method.","metadata":{"id":"p68j6XxEvP9K"}},{"cell_type":"code","source":"# Reset states generated by Keras\ntf.keras.backend.clear_session()\n\n# Build the model\nmodel = tf.keras.models.Sequential([\n    tf.keras.Input(shape=(window_size, 1)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    tf.keras.layers.Dense(1),\n    tf.keras.layers.Lambda(lambda x: x * 100.0)\n])\n\n# Set the learning rate\nlearning_rate = 2e-6\n\n# Set the optimizer \noptimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n\n# Set the training parameters\nmodel.compile(loss=tf.keras.losses.Huber(),\n              optimizer=optimizer,\n              metrics=[\"mae\"])\n\n# Train the model\nhistory = model.fit(dataset,epochs=100, validation_data=val_set)","metadata":{"id":"wujKz6tXDSn0","trusted":true},"outputs":[],"execution_count":null}]}